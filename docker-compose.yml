version: '3'

services:
  # Backend API service
  backend:
    build:
      context: .
    ports:
      - "5001:5001"
    volumes:
      - ./data:/app/data
    restart: unless-stopped
    environment:
      - FLASK_ENV=production
      - MODEL_NAME=gemma3
      - EMBEDDING_MODEL=BAAI/bge-small-en-v1.5
      - VECTOR_DB_PATH=/app/data/chroma_db
    depends_on:
      - ollama

  # Ollama service for running LLMs
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ./ollama:/root/.ollama
    restart: unless-stopped
    # # Using Metal for MacOS acceleration
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: gpu
    #           capabilities: [metal]
    #           count: 1